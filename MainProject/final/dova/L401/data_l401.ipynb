{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3be72d7d",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "Read the NeTEx files and extract all unique journeys on a line. This is every journeynumber per operatingday. The TimeDemandTypeRef is found for every journey. This is later on used to connect each journey with the scheduled driving times between the stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab444e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "files = [\n",
    "    '../../../data/NeTEx/NeTEx_CXX_SRE_20250527_2025-06-01_202500014_baseline.xml',\n",
    "    '../../../data/NeTEx/NeTEx_CXX_SRE_20250702_2025-07-06_202500015_baseline.xml',\n",
    "    '../../../data/NeTEx/NeTEx_CXX_SRE_20250717_2025-07-20_202500016_baseline.xml',\n",
    "    '../../../data/NeTEx/NeTEx_CXX_SRE_20250731_2025-08-03_202500018_baseline.xml',\n",
    "    '../../../data/NeTEx/NeTEx_CXX_SRE_20250812_2025-08-17_202500019_baseline.xml',\n",
    "]\n",
    "\n",
    "LINE_PREFIX = \"CXX:ServiceJourney:L401-\"\n",
    "\n",
    "def parse_filename_meta(path: str):\n",
    "    \"\"\"\n",
    "    From filename like:\n",
    "    NeTEx_CXX_SRE_20250527_2025-06-01_202500014_baseline.xml\n",
    "    -> export_created_yyyymmdd=20250527, export_validfrom=2025-06-01, export_version=202500014\n",
    "    \"\"\"\n",
    "    name = Path(path).name\n",
    "    m = re.search(r'NeTEx_.*_(\\d{8})_(\\d{4}-\\d{2}-\\d{2})_(\\d{9})_baseline\\.xml', name)\n",
    "    if not m:\n",
    "        return None, None, None\n",
    "    created = dt.datetime.strptime(m.group(1), \"%Y%m%d\").date()\n",
    "    validfrom = dt.date.fromisoformat(m.group(2))\n",
    "    version = int(m.group(3))\n",
    "    return created, validfrom, version\n",
    "\n",
    "def parse_dep_time(dep_time_str: str):\n",
    "    try:\n",
    "        return dt.time.fromisoformat(dep_time_str)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def build_tables(root):\n",
    "    # namespace\n",
    "    ns = ''\n",
    "    if root.tag.startswith('{'):\n",
    "        ns = root.tag.split('}')[0].strip('{')\n",
    "        nsmap = {'ns': ns}\n",
    "    else:\n",
    "        nsmap = {}\n",
    "\n",
    "    def t(tag: str) -> str:\n",
    "        return f'{{{ns}}}{tag}' if ns else tag\n",
    "\n",
    "    # AvailabilityCondition lookup\n",
    "    availability = {}\n",
    "    for ac in root.findall(f'.//{t(\"AvailabilityCondition\")}', nsmap):\n",
    "        ac_id = ac.get('id')\n",
    "        if not ac_id:\n",
    "            continue\n",
    "        from_dt = (ac.findtext(t('FromDate')) if ns else ac.findtext('FromDate')) or \"\"\n",
    "        to_dt   = (ac.findtext(t('ToDate')) if ns else ac.findtext('ToDate')) or \"\"\n",
    "        bits_raw = (ac.findtext(t('ValidDayBits')) if ns else ac.findtext('ValidDayBits')) or \"\"\n",
    "        bits = \"\".join(bits_raw.split())\n",
    "        availability[ac_id] = {\"from_dt\": from_dt, \"to_dt\": to_dt, \"bits\": bits}\n",
    "\n",
    "    def expand_availability_dates(ac_id: str) -> list[dt.date]:\n",
    "        info = availability.get(ac_id)\n",
    "        if not info:\n",
    "            return []\n",
    "        from_dt, to_dt, bits = info[\"from_dt\"], info[\"to_dt\"], info[\"bits\"]\n",
    "        if not from_dt or not to_dt or not bits:\n",
    "            return []\n",
    "\n",
    "        start = dt.date.fromisoformat(from_dt[:10])\n",
    "        to_date = dt.date.fromisoformat(to_dt[:10])\n",
    "\n",
    "        # handle midnight end boundary as end-exclusive (common)\n",
    "        to_time_part = to_dt[11:19] if len(to_dt) >= 19 else None\n",
    "        end_inclusive = to_date - dt.timedelta(days=1) if to_time_part == \"00:00:00\" else to_date\n",
    "        if end_inclusive < start:\n",
    "            return []\n",
    "\n",
    "        days_inclusive = (end_inclusive - start).days + 1\n",
    "\n",
    "        # Case A: 7-bit weekday mask Mon..Sun\n",
    "        if len(bits) == 7:\n",
    "            out = []\n",
    "            cur = start\n",
    "            while cur <= end_inclusive:\n",
    "                if bits[cur.weekday()] == \"1\":\n",
    "                    out.append(cur)\n",
    "                cur += dt.timedelta(days=1)\n",
    "            return out\n",
    "\n",
    "        # Case B: per-day bitstring from start\n",
    "        n = min(days_inclusive, len(bits))\n",
    "        return [start + dt.timedelta(days=i) for i in range(n) if bits[i] == \"1\"]\n",
    "\n",
    "    # Extract ServiceJourneys for the line\n",
    "    rows = []\n",
    "    for sj in root.findall(f'.//{t(\"ServiceJourney\")}', nsmap):\n",
    "        sj_id = sj.get('id', '')\n",
    "        if not sj_id.startswith(LINE_PREFIX):\n",
    "            continue\n",
    "\n",
    "        dep_time = sj.findtext(t('DepartureTime')) if ns else sj.findtext('DepartureTime')\n",
    "        dep_t = parse_dep_time(dep_time or \"\")\n",
    "        if dep_t is None:\n",
    "            continue\n",
    "\n",
    "        # JourneyNumber\n",
    "        private_code = sj.find(t('PrivateCode')) if ns else sj.find('PrivateCode')\n",
    "        journey_number = (\n",
    "            private_code.text\n",
    "            if private_code is not None and private_code.get('type') == 'JourneyNumber'\n",
    "            else None\n",
    "        )\n",
    "        if not journey_number:\n",
    "            continue\n",
    "\n",
    "        # TimeDemandTypeRef\n",
    "        tdt = sj.find(t('TimeDemandTypeRef')) if ns else sj.find('TimeDemandTypeRef')\n",
    "        tdt_ref = tdt.get('ref') if tdt is not None else None\n",
    "        if not tdt_ref:\n",
    "            continue\n",
    "\n",
    "        # ServiceJourneyPatternRef (helps dedupe correctly)\n",
    "        sjp = sj.find(t('ServiceJourneyPatternRef')) if ns else sj.find('ServiceJourneyPatternRef')\n",
    "        sjp_ref = sjp.get('ref') if sjp is not None else None\n",
    "\n",
    "        # AvailabilityConditionRef\n",
    "        ac = sj.find(f'.//{t(\"AvailabilityConditionRef\")}') if ns else sj.find('.//AvailabilityConditionRef')\n",
    "        ac_ref = ac.get('ref') if ac is not None else None\n",
    "        if not ac_ref:\n",
    "            continue\n",
    "\n",
    "        dates = expand_availability_dates(ac_ref)\n",
    "\n",
    "        for d in dates:\n",
    "            rows.append({\n",
    "                \"ServiceJourneyId\": sj_id,\n",
    "                \"ServiceJourneyPatternRef\": sjp_ref,\n",
    "                \"JourneyNumber\": journey_number,\n",
    "                \"TimeDemandTypeRef\": tdt_ref,\n",
    "                \"AvailabilityConditionRef\": ac_ref,\n",
    "                \"DepartureDateTime\": dt.datetime.combine(d, dep_t),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "all_expanded = []\n",
    "\n",
    "for path in files:\n",
    "    created, validfrom, file_version = parse_filename_meta(path)\n",
    "\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    df_expanded = build_tables(root)\n",
    "\n",
    "    df_expanded[\"SourceFile\"] = Path(path).name\n",
    "    df_expanded[\"ExportCreatedDate\"] = created\n",
    "    df_expanded[\"ExportValidFrom\"] = validfrom\n",
    "    df_expanded[\"ExportVersion\"] = file_version\n",
    "\n",
    "    all_expanded.append(df_expanded)\n",
    "\n",
    "combined = pd.concat(all_expanded, ignore_index=True)\n",
    "\n",
    "# --- Keep only the latest export for overlapping trips ---\n",
    "# Define “same trip” key:\n",
    "# pattern + journey number + dated departure is usually stable enough.\n",
    "dedupe_key = [\"ServiceJourneyPatternRef\", \"JourneyNumber\", \"DepartureDateTime\"]\n",
    "\n",
    "combined_latest = (\n",
    "    combined.sort_values(\n",
    "        [\"DepartureDateTime\", \"ExportCreatedDate\", \"ExportVersion\"],\n",
    "        ascending=[True, False, False]\n",
    "    )\n",
    "    .drop_duplicates(subset=dedupe_key, keep=\"first\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# print(\"Combined rows:\", len(combined))\n",
    "# print(\"After keeping latest per trip:\", len(combined_latest))\n",
    "\n",
    "combined_latest.to_csv('dova_l401_servicejourneys_expanded_latest.csv', index=False)\n",
    "combined_latest.head(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4642912",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "The goal of this part is to get the scheduled driving times between all stops for all TimeDemandTypeRefs. This can then later be connected to the journeys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babe6d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract TimeDemandType -> ordered stop-to-stop runtimes, collapsing TimingPoint segments:\n",
    "#   Stop -> TimingPoint -> Stop  ==> Stop -> Stop with summed runtime\n",
    "# Also fixes scrambled XML order by ordering edges into a path using TimingLinkRef ids.\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def parse_duration_to_seconds(iso: str) -> int | None:\n",
    "    if not iso:\n",
    "        return None\n",
    "    iso = iso.strip()\n",
    "    m = re.fullmatch(r'PT(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?', iso)\n",
    "    if not m:\n",
    "        return None\n",
    "    h = int(m.group(1) or 0)\n",
    "    mi = int(m.group(2) or 0)\n",
    "    s = int(m.group(3) or 0)\n",
    "    return h * 3600 + mi * 60 + s\n",
    "\n",
    "def local(tag: str) -> str:\n",
    "    return tag.split(\"}\", 1)[1] if \"}\" in tag else tag\n",
    "\n",
    "def order_raw_edges_into_path(raw_edges: list[tuple[str, str, int, str]]):\n",
    "    \"\"\"\n",
    "    raw_edges: list of (from_ref, to_ref, runtime_sec, link_id)\n",
    "    Returns ordered list of edges in path order (same tuple form).\n",
    "    Assumes no branches.\n",
    "    \"\"\"\n",
    "    if not raw_edges:\n",
    "        return []\n",
    "\n",
    "    nodes = set()\n",
    "    in_deg = {}\n",
    "    out_map = {}\n",
    "\n",
    "    for a, b, rt, lid in raw_edges:\n",
    "        nodes.add(a); nodes.add(b)\n",
    "        in_deg[b] = in_deg.get(b, 0) + 1\n",
    "        in_deg.setdefault(a, in_deg.get(a, 0))\n",
    "        out_map.setdefault(a, []).append((b, rt, lid))\n",
    "\n",
    "    starts = [n for n in nodes if in_deg.get(n, 0) == 0]\n",
    "    start = starts[0] if starts else raw_edges[0][0]\n",
    "\n",
    "    ordered = []\n",
    "    cur = start\n",
    "    used_links = set()\n",
    "    max_steps = len(raw_edges) + 5  # safety\n",
    "\n",
    "    steps = 0\n",
    "    while steps < max_steps:\n",
    "        steps += 1\n",
    "        nxt = None\n",
    "        for b, rt, lid in out_map.get(cur, []):\n",
    "            if lid not in used_links:\n",
    "                nxt = (b, rt, lid)\n",
    "                break\n",
    "        if nxt is None:\n",
    "            break\n",
    "        b, rt, lid = nxt\n",
    "        used_links.add(lid)\n",
    "        ordered.append((cur, b, rt, lid))\n",
    "        cur = b\n",
    "        if len(ordered) == len(raw_edges):\n",
    "            break\n",
    "\n",
    "    return ordered\n",
    "\n",
    "def collapse_timingpoints_to_stops(ordered_edges: list[tuple[str, str, int, str]]):\n",
    "    \"\"\"\n",
    "    ordered_edges: (from_ref, to_ref, runtime_sec, link_id) in path order.\n",
    "    refs may be ScheduledStopPoint or TimingPoint.\n",
    "\n",
    "    Collapses:\n",
    "      Stop -> TP -> Stop into Stop -> Stop (sum runtimes)\n",
    "    Also handles endpoint TP segments by attaching them to adjacent stop segment when possible.\n",
    "\n",
    "    Returns:\n",
    "      stop_edges: list of (from_stop_ref, to_stop_ref, runtime_sec, link_ids_used)\n",
    "    \"\"\"\n",
    "    def is_stop(x: str) -> bool:\n",
    "        return x is not None and (\":ScheduledStopPoint:\" in x)\n",
    "\n",
    "    def is_tp(x: str) -> bool:\n",
    "        return x is not None and (\":TimingPoint:\" in x)\n",
    "\n",
    "    stop_edges = []\n",
    "    i = 0\n",
    "    while i < len(ordered_edges):\n",
    "        a, b, rt, lid = ordered_edges[i]\n",
    "\n",
    "        # Stop -> TimingPoint\n",
    "        if is_stop(a) and is_tp(b):\n",
    "            # If next edge is TimingPoint -> Stop, collapse both into Stop->Stop\n",
    "            if i + 1 < len(ordered_edges):\n",
    "                a2, b2, rt2, lid2 = ordered_edges[i + 1]\n",
    "                if a2 == b and is_stop(b2):\n",
    "                    stop_edges.append((a, b2, rt + rt2, [lid, lid2]))\n",
    "                    i += 2\n",
    "                    continue\n",
    "            # Endpoint Stop->TP without a TP->Stop after it: skip (or keep separately if you want)\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # TimingPoint -> Stop (start-of-trip timing point)\n",
    "        if is_tp(a) and is_stop(b):\n",
    "            # If next edge starts at that stop, attach this runtime to the next stop->(something) edge\n",
    "            if i + 1 < len(ordered_edges):\n",
    "                a2, b2, rt2, lid2 = ordered_edges[i + 1]\n",
    "                # Ideally Stop -> Stop next\n",
    "                if a2 == b and is_stop(a2) and is_stop(b2):\n",
    "                    stop_edges.append((a2, b2, rt + rt2, [lid, lid2]))\n",
    "                    i += 2\n",
    "                    continue\n",
    "                # Or Stop -> TP next, which will then be handled by Stop->TP->Stop collapse\n",
    "                if a2 == b and is_stop(a2) and is_tp(b2):\n",
    "                    # Let the Stop->TP->Stop collapse handle it, but carry runtime forward by\n",
    "                    # replacing next edge runtime with rt+rt2:\n",
    "                    ordered_edges[i + 1] = (a2, b2, rt + rt2, lid2)\n",
    "                    i += 1\n",
    "                    continue\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Normal Stop -> Stop\n",
    "        if is_stop(a) and is_stop(b):\n",
    "            stop_edges.append((a, b, rt, [lid]))\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # TP -> TP or unknown: skip\n",
    "        i += 1\n",
    "\n",
    "    return stop_edges\n",
    "\n",
    "def extract_tdt_runtimes_and_stops_collapsed(xml_path: str) -> dict[str, dict]:\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # namespace detect\n",
    "    ns = ''\n",
    "    if root.tag.startswith('{'):\n",
    "        ns = root.tag.split('}')[0].strip('{')\n",
    "        nsmap = {'ns': ns}\n",
    "    else:\n",
    "        nsmap = {}\n",
    "\n",
    "    def t(tag: str) -> str:\n",
    "        return f'{{{ns}}}{tag}' if ns else tag\n",
    "\n",
    "    # Index elements by @id so TimingLinkRef can be resolved quickly\n",
    "    id_index = {}\n",
    "    for el in root.iter():\n",
    "        eid = el.get(\"id\")\n",
    "        if eid:\n",
    "            id_index[eid] = el\n",
    "\n",
    "    def get_from_to_refs(link_el):\n",
    "        if link_el is None:\n",
    "            return None, None\n",
    "        from_ref = None\n",
    "        to_ref = None\n",
    "        for child in link_el.iter():\n",
    "            cl = local(child.tag)\n",
    "            if cl in (\"FromPointRef\", \"FromStopPointInJourneyPatternRef\") and child.get(\"ref\"):\n",
    "                from_ref = child.get(\"ref\")\n",
    "            elif cl in (\"ToPointRef\", \"ToStopPointInJourneyPatternRef\") and child.get(\"ref\"):\n",
    "                to_ref = child.get(\"ref\")\n",
    "            if from_ref and to_ref:\n",
    "                break\n",
    "        return from_ref, to_ref\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    for tdt_el in root.findall(f'.//{t(\"TimeDemandType\")}', nsmap):\n",
    "        tdt_id = tdt_el.get(\"id\")\n",
    "        if not tdt_id:\n",
    "            continue\n",
    "\n",
    "        raw_edges = []  # (from_ref, to_ref, runtime_sec, link_id)\n",
    "\n",
    "        for jrt in tdt_el.findall(f'.//{t(\"JourneyRunTime\")}', nsmap):\n",
    "            sec = parse_duration_to_seconds(jrt.findtext(t(\"RunTime\")))\n",
    "            if sec is None:\n",
    "                continue\n",
    "\n",
    "            tlr = jrt.find(t(\"TimingLinkRef\"))\n",
    "            link_ref = tlr.get(\"ref\") if tlr is not None else None\n",
    "            if not link_ref:\n",
    "                continue\n",
    "\n",
    "            link_el = id_index.get(link_ref)\n",
    "            fref, toref = get_from_to_refs(link_el)\n",
    "            if not fref or not toref:\n",
    "                continue\n",
    "\n",
    "            raw_edges.append((fref, toref, sec, link_ref))\n",
    "\n",
    "        # 1) order the raw timing-link edges into path order\n",
    "        ordered_edges = order_raw_edges_into_path(raw_edges)\n",
    "\n",
    "        # 2) collapse TimingPoint segments into stop-to-stop segments\n",
    "        stop_edges = collapse_timingpoints_to_stops(ordered_edges)\n",
    "\n",
    "        # Build StopCodes + RunTimesSeconds arrays\n",
    "        if stop_edges:\n",
    "            stop_codes = [stop_edges[0][0]] + [e[1] for e in stop_edges]\n",
    "            run_times = [e[2] for e in stop_edges]\n",
    "        else:\n",
    "            stop_codes = []\n",
    "            run_times = []\n",
    "\n",
    "        out[tdt_id] = {\n",
    "            \"RunTimesSeconds\": run_times,\n",
    "            \"StopCodes\": stop_codes,\n",
    "            \"n_raw_segments\": len(raw_edges),\n",
    "            \"n_ordered_segments\": len(ordered_edges),\n",
    "            \"n_collapsed_segments\": len(run_times),\n",
    "        }\n",
    "\n",
    "    return out\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# BUILD MASTER MAP ACROSS MULTIPLE FILES (don't overwrite non-empty with empty)\n",
    "# Requires:\n",
    "#   files: list[str]          # your 5 NeTEx XML paths\n",
    "#   combined_latest: DataFrame with TimeDemandTypeRef (from your earlier step)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "tdt_master = {}\n",
    "for path in files:\n",
    "    tdt_map = extract_tdt_runtimes_and_stops_collapsed(path)\n",
    "    for k, v in tdt_map.items():\n",
    "        new_len = len(v.get(\"RunTimesSeconds\") or [])\n",
    "        old_len = len(tdt_master.get(k, {}).get(\"RunTimesSeconds\") or [])\n",
    "        if k not in tdt_master or (old_len == 0 and new_len > 0):\n",
    "            tdt_master[k] = v\n",
    "\n",
    "# Build df for the TimeDemandTypes actually used\n",
    "unique_tdts = combined_latest[\"TimeDemandTypeRef\"].dropna().unique()\n",
    "\n",
    "rows = []\n",
    "for ref in unique_tdts:\n",
    "    entry = tdt_master.get(ref)\n",
    "    rows.append({\n",
    "        \"TimeDemandTypeRef\": ref,\n",
    "        \"RunTimesSeconds\": entry[\"RunTimesSeconds\"] if entry else None,\n",
    "        \"StopCodes\": entry[\"StopCodes\"] if entry else None,\n",
    "        \"n_segments\": len(entry[\"RunTimesSeconds\"]) if entry else 0,\n",
    "        \"n_stops\": len(entry[\"StopCodes\"]) if entry else 0,\n",
    "        \"n_raw_segments\": entry.get(\"n_raw_segments\") if entry else None,\n",
    "        \"n_ordered_segments\": entry.get(\"n_ordered_segments\") if entry else None,\n",
    "        \"missing_definition\": entry is None\n",
    "    })\n",
    "\n",
    "tdt_df = (\n",
    "    pd.DataFrame(rows)\n",
    "      .sort_values([\"missing_definition\", \"TimeDemandTypeRef\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "# keep only last numeric part of StopCodes (e.g., \"CXX:ScheduledStopPoint:68601010\" -> \"68601010\") ---\n",
    "def _last_number(x):\n",
    "    if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "        return None\n",
    "    s = str(x)\n",
    "    m = re.search(r'(\\d+)\\s*$', s)  # trailing digits\n",
    "    return m.group(1) if m else s   # fallback: leave as-is if no trailing digits\n",
    "\n",
    "def _clean_stopcodes_list(lst):\n",
    "    if lst is None or (isinstance(lst, float) and pd.isna(lst)):\n",
    "        return None\n",
    "    if not isinstance(lst, list):\n",
    "        return lst\n",
    "    return [_last_number(v) for v in lst]\n",
    "\n",
    "tdt_df[\"StopCodes\"] = tdt_df[\"StopCodes\"].apply(_clean_stopcodes_list)\n",
    "\n",
    "# Sanity: for non-empty, stops should be segments + 1\n",
    "bad = tdt_df.query(\"n_segments > 0 and n_stops != n_segments + 1\")\n",
    "print(\"Broken segment/stop alignment:\", len(bad))\n",
    "\n",
    "tdt_df[\n",
    "    [\"TimeDemandTypeRef\", \"RunTimesSeconds\", \"StopCodes\"]\n",
    "].to_csv('dova_l401_timedemandtype_runtimes_collapsed.csv', index=False)\n",
    "tdt_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b59f84",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "In this part the data from the previous parts are connected. The final csv file contains the following columns: JourneyNumber,DepartureDateTime,RunTimesSeconds,StopCodes. This data can be connected with the KV6 messages. This connection happens based on the JourneyNumber and departure date. It can explain for every message in the KV6 dataset what the planned driving times are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cebdc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final dataset\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "\n",
    "df1 = read_csv('dova_l401_servicejourneys_expanded_latest.csv')\n",
    "df2 = read_csv('dova_l401_timedemandtype_runtimes_collapsed.csv')\n",
    "\n",
    "# Final format: merge df1 and df2 on TimeDemandTypeRef. Include per row: JourneyNumber, DepartureDateTime, OperatingDay, RunTimesSeconds, StopCodes\n",
    "final_df = pd.merge(\n",
    "    df1,\n",
    "    df2,\n",
    "    on='TimeDemandTypeRef',\n",
    "    how='left'\n",
    " )\n",
    "\n",
    "final_df[\"DepartureDateTime\"] = pd.to_datetime(final_df[\"DepartureDateTime\"], errors=\"coerce\")\n",
    "final_df[\"OperatingDay\"] = final_df[\"DepartureDateTime\"].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "final_df = final_df[[\"TimeDemandTypeRef\", \"JourneyNumber\", \"DepartureDateTime\", \"OperatingDay\", \"RunTimesSeconds\", \"StopCodes\"]]\n",
    "\n",
    "final_df.to_csv('dova_l401_data.csv', index=False)\n",
    "final_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38345c7f",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "Merge Line 401 messages exp with the dova dataset from step 3. Joining takes place based on JourneyNumber and OperatingDay. The final df contains all data from messages_exp with added the RunTimeSeconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230dc91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4 - Merge Line 401 messages_exp with DOVA dataset (Part 3)\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "dova_path = Path('dova_l401_data.csv')\n",
    "msg_path = Path('../../input/line401_combinations_exp.csv')\n",
    "\n",
    "df_dova = pd.read_csv(dova_path)\n",
    "df_msg = pd.read_csv(msg_path)\n",
    "\n",
    "def _col_lookup(df: pd.DataFrame) -> dict[str, str]:\n",
    "    return {str(c).strip().lower(): c for c in df.columns}\n",
    "\n",
    "# --- Messages: standardize JourneyNumber + OperatingDay ---\n",
    "msg_cols = _col_lookup(df_msg)\n",
    "jn_col = msg_cols.get('journeynumber')\n",
    "if not jn_col:\n",
    "    raise KeyError(\"messages_exp is missing JourneyNumber (or JourneyNumberr)\")\n",
    "df_msg['JourneyNumber'] = df_msg[jn_col].astype(str)\n",
    "\n",
    "op_col = msg_cols.get('operatingday')\n",
    "if op_col:\n",
    "    df_msg['OperatingDay'] = pd.to_datetime(df_msg[op_col], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "else:\n",
    "    dt_candidates = ['DepartureDateTime', 'departuredatetime', 'Timestamp', 'timestamp', 'MessageDateTime', 'messagedatetime', 'DateTime', 'datetime', 'TimeStamp', 'timestamp']\n",
    "    dt_col = next((msg_cols.get(c.lower()) for c in dt_candidates if msg_cols.get(c.lower())), None)\n",
    "    if not dt_col:\n",
    "        raise KeyError(\"messages_exp is missing OperatingDay and no known datetime column was found\")\n",
    "    df_msg['OperatingDay'] = pd.to_datetime(df_msg[dt_col], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# --- DOVA: standardize JourneyNumber + OperatingDay ---\n",
    "dova_cols = _col_lookup(df_dova)\n",
    "if 'journeynumber' not in dova_cols or 'operatingday' not in dova_cols:\n",
    "    raise KeyError(\"dova_l401_data.csv must contain JourneyNumber and OperatingDay\")\n",
    "df_dova['JourneyNumber'] = df_dova[dova_cols['journeynumber']].astype(str)\n",
    "df_dova['OperatingDay'] = pd.to_datetime(df_dova[dova_cols['operatingday']], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Merge (keep all messages rows; attach planned runtimes)\n",
    "merged_df = df_msg.merge(\n",
    "    df_dova[['JourneyNumber', 'OperatingDay','StopCodes', 'RunTimesSeconds', 'TimeDemandTypeRef']],\n",
    "    on=['JourneyNumber', 'OperatingDay'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "merged_df.drop([\"JourneyNumber\", \"OperatingDay\"], axis=1, inplace=True)\n",
    "merged_df.to_csv('line401_combinations_exp_with_dova.csv', index=False)\n",
    "merged_df.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa3ad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the running time segments for a specific journey:\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('line401_combinations_exp_with_dova.csv');\n",
    "# Filter dataset for 02-06-2025\n",
    "df = df[(df['operatingday'] == '2025-06-09') & (df['to_station'] == 1) & (df['journeynumber'] == 7075)]\n",
    "df.head(20);\n",
    "\n",
    "# These running times can be used for example for predictions of delays."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
